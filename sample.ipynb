{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Libraries",
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import abc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.datasets as datasets\n",
    "from typing import List, Tuple"
   ],
   "id": "fbc121e30a2defb3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Aufgabe 1",
   "id": "4184632739fe6ad"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Installieren der MNIST-Datasets",
   "id": "d7ad8005c457352"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True)"
   ],
   "id": "462120a1c0337193"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Aufgabe 3",
   "id": "d628c80bc1feeb40"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Linear Layer",
   "id": "428135e91a041a87"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class LinearLayer:\n",
    "    def __init__(self, in_features: int, out_features: int, bias: bool = True, learning_rate: float = 0.01):\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.bias = bias\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # Xavier initialization works better for this task\n",
    "        self.weights = np.random.uniform(\n",
    "            -np.sqrt(6.0 / (in_features + out_features)),\n",
    "            np.sqrt(6.0 / (in_features + out_features)),\n",
    "            size=(out_features, in_features)\n",
    "        )\n",
    "\n",
    "        # Initialize bias to small values, not zeros\n",
    "        self.bias_term = np.random.uniform(-0.01, 0.01, size=out_features) if bias else None\n",
    "\n",
    "        # Gradients\n",
    "        self.grad_weights = None\n",
    "        self.grad_bias = None\n",
    "        self.input_cache = None # Für den Backward-Pass merken wir uns den Input\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Führt die lineare Transformation y = xW^T + b durch.\n",
    "        :param x: Input-Matrix (batch_size x in_features)\n",
    "        :return: Transformierte Matrix (batch_size x out_features)\n",
    "        \"\"\"\n",
    "        # Make sure input is 2D\n",
    "        if len(x.shape) == 1:\n",
    "            x = x.reshape(1, -1)\n",
    "\n",
    "        self.input_cache = x\n",
    "        y = np.dot(x, self.weights.T)\n",
    "        if self.bias:\n",
    "            y += self.bias_term\n",
    "        return y\n",
    "\n",
    "\n",
    "    def backward(self, grad_output: np.ndarray) -> np.ndarray:\n",
    "        # Ensure inputs are properly shaped\n",
    "        if len(grad_output.shape) == 1:\n",
    "            grad_output = grad_output.reshape(1, -1)\n",
    "        if len(self.input_cache.shape) == 1:\n",
    "            self.input_cache = self.input_cache.reshape(1, -1)\n",
    "\n",
    "        batch_size = grad_output.shape[0]\n",
    "\n",
    "        # Correctly compute weight gradients\n",
    "        self.grad_weights = np.dot(grad_output.T, self.input_cache) / batch_size\n",
    "\n",
    "        # Clip gradients at a higher threshold to allow learning\n",
    "        grad_clip_value = 5.0  # Increased from 1.0\n",
    "        self.grad_weights = np.clip(self.grad_weights, -grad_clip_value, grad_clip_value)\n",
    "\n",
    "        if self.bias:\n",
    "            self.grad_bias = np.mean(grad_output, axis=0)\n",
    "            self.grad_bias = np.clip(self.grad_bias, -grad_clip_value, grad_clip_value)\n",
    "\n",
    "        # Gradient for previous layer\n",
    "        grad_input = np.dot(grad_output, self.weights)\n",
    "\n",
    "        return grad_input\n",
    "\n",
    "\n",
    "    def update_parameters(self) -> None:\n",
    "        \"\"\"\n",
    "        Führt das Update der Gewichte und des Bias durch (Gradient Descent).\n",
    "        \"\"\"\n",
    "        self.weights -= self.learning_rate * self.grad_weights\n",
    "        if self.bias:\n",
    "            self.bias_term -= self.learning_rate * self.grad_bias"
   ],
   "id": "59c2e2d07a4bd5e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Activation Functions",
   "id": "c49e41d66f861efe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class ActivationFunction(abc.ABC):\n",
    "    @abc.abstractmethod\n",
    "    def activate(self, x: np.ndarray) -> np.ndarray:\n",
    "        ...\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def derivative(self, x: np.ndarray) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "class ReLU(ActivationFunction):\n",
    "    def __init__(self):\n",
    "        self.input_cache = None\n",
    "\n",
    "    def activate(self, x: np.ndarray) -> np.ndarray:\n",
    "        self.input_cache = x\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def derivative(self, x: np.ndarray) -> np.ndarray:\n",
    "        return np.where(self.input_cache > 0, 1, 0)\n",
    "\n",
    "class Sigmoid(ActivationFunction):\n",
    "    def __init__(self):\n",
    "        self.output_cache = None\n",
    "\n",
    "    def activate(self, x: np.ndarray) -> np.ndarray:\n",
    "        # Clip values to avoid overflow\n",
    "        x = np.clip(x, -500, 500)\n",
    "        output = 1 / (1 + np.exp(-x))\n",
    "        self.output_cache = output\n",
    "        return output\n",
    "\n",
    "    def derivative(self, x: np.ndarray) -> np.ndarray:\n",
    "        # Sigmoid derivative is sigmoid(x) * (1 - sigmoid(x))\n",
    "        return self.output_cache * (1 - self.output_cache)"
   ],
   "id": "62070df4c082df8b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Loss Functions",
   "id": "d6f9c37c6b25e065"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class LossFunction(abc.ABC):\n",
    "    @abc.abstractmethod\n",
    "    def loss(self, predictions: np.ndarray, targets: np.ndarray) -> float:\n",
    "        ...\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def gradient(self, predictions: np.ndarray, targets: np.ndarray) -> np.ndarray:\n",
    "        ...\n",
    "\n",
    "def softmax(x: np.ndarray) -> np.ndarray:\n",
    "    # Convert to numpy array if not already\n",
    "    x_array = np.asarray(x)\n",
    "    if len(x_array.shape) == 1:\n",
    "        x_array = x_array.reshape(1, -1)\n",
    "\n",
    "    # Numerical stability: subtract the maximum from each entry\n",
    "    shifted_x = x_array - np.max(x_array, axis=1, keepdims=True)\n",
    "\n",
    "    # Clip to prevent overflow\n",
    "    shifted_x = np.clip(shifted_x, -500, 500)\n",
    "\n",
    "    exps = np.exp(shifted_x)\n",
    "    return exps / np.sum(exps, axis=1, keepdims=True)\n",
    "\n",
    "class CrossEntropyLoss(LossFunction):\n",
    "    def loss(self, predictions: np.ndarray, targets: np.ndarray) -> float:\n",
    "        targets = np.asarray(targets)\n",
    "        probs = softmax(predictions)\n",
    "        # Avoid division by zero\n",
    "        epsilon = 1e-15\n",
    "        probs = np.clip(probs, epsilon, 1 - epsilon)\n",
    "        batch_loss = -np.sum(targets * np.log(probs)) / targets.shape[0]\n",
    "        return batch_loss\n",
    "\n",
    "    def gradient(self, predictions: np.ndarray, targets: np.ndarray) -> np.ndarray:\n",
    "        targets = np.asarray(targets)\n",
    "        m = targets.shape[0]\n",
    "        probs = softmax(predictions)\n",
    "        grad = (probs - targets) / m\n",
    "        # Option 1: Remove clipping entirely:\n",
    "        # grad = (probs - targets) / m\n",
    "        # Option 2: Relax clipping range, e.g.:\n",
    "        # grad = np.clip(grad, -5.0, 5.0)\n",
    "        return grad\n"
   ],
   "id": "87f7583fd798e83c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Neuronal Network",
   "id": "fdb00f04a3b6a5bf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, layers: List[LinearLayer], activation_functions: List[ActivationFunction] = None,\n",
    "                 loss_function: LossFunction = CrossEntropyLoss()) -> None:\n",
    "        self.layers = layers\n",
    "\n",
    "        # Allow different activation functions per layer\n",
    "        if activation_functions is None:\n",
    "            # Default to ReLU for all layers except last (no activation for output layer)\n",
    "            self.activation_functions = [ReLU() for _ in range(len(layers) - 1)]\n",
    "            self.activation_functions.append(None)  # No activation for last layer\n",
    "        else:\n",
    "            self.activation_functions = activation_functions\n",
    "\n",
    "        self.loss_function = loss_function\n",
    "        self.activations_cache = []\n",
    "\n",
    "    def forward(self, data: np.ndarray) -> np.ndarray:\n",
    "        # Ensure data is 2D\n",
    "        if len(data.shape) == 1:\n",
    "            data = data.reshape(1, -1)\n",
    "\n",
    "        output = data\n",
    "        self.activations_cache = [output]\n",
    "\n",
    "        for i, (layer, activation) in enumerate(zip(self.layers, self.activation_functions)):\n",
    "            output = layer.forward(output)\n",
    "            if activation is not None:\n",
    "                output = activation.activate(output)\n",
    "            self.activations_cache.append(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backward(self, grad_output: np.ndarray) -> None:\n",
    "        for i in reversed(range(len(self.layers))):\n",
    "            layer = self.layers[i]\n",
    "            activation = self.activation_functions[i]\n",
    "\n",
    "            if activation is not None:\n",
    "                # Apply activation gradient\n",
    "                activation_input = self.activations_cache[i + 1]  # Output after linear layer\n",
    "                grad_output = grad_output * activation.derivative(activation_input)\n",
    "\n",
    "            grad_output = layer.backward(grad_output)\n",
    "\n",
    "    def update_parameters(self, learning_rate_scheduler=None) -> None:\n",
    "        \"\"\"Update parameters with optional learning rate scheduling\"\"\"\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if learning_rate_scheduler:\n",
    "                layer.learning_rate = learning_rate_scheduler(layer.learning_rate, i)\n",
    "            layer.update_parameters()\n",
    "\n",
    "    def compute_loss(self, predictions: np.ndarray, targets: np.ndarray) -> float:\n",
    "        return self.loss_function.loss(predictions, targets)\n",
    "\n",
    "    def train_batch(self, X_batch: np.ndarray, y_batch: np.ndarray) -> float:\n",
    "        # Forward pass\n",
    "        predictions = self.forward(X_batch)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = self.compute_loss(predictions, y_batch)\n",
    "\n",
    "        # Compute gradients\n",
    "        grad_loss = self.loss_function.gradient(predictions, y_batch)\n",
    "\n",
    "        # Backward pass\n",
    "        self.backward(grad_loss)\n",
    "\n",
    "        # Update parameters\n",
    "        self.update_parameters()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def evaluate(self, X: np.ndarray, y: np.ndarray) -> Tuple[float, float]:\n",
    "        # Forward pass\n",
    "        predictions = self.forward(X)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = self.compute_loss(predictions, y)\n",
    "\n",
    "        # Compute accuracy\n",
    "        y_pred = np.argmax(softmax(predictions), axis=1)\n",
    "        y_true = np.argmax(y, axis=1)\n",
    "        accuracy = np.mean(y_pred == y_true)\n",
    "\n",
    "        return loss, accuracy"
   ],
   "id": "ec4962c84df85df5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Setup",
   "id": "857beabc8257fca3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def train_network(network, train, num_epochs=10, batch_size=128, initial_lr=0.05):\n",
    "    num_samples = len(train)\n",
    "    losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        batch_count = 0\n",
    "\n",
    "        # Learning rate decay (milder decay)\n",
    "        current_lr = initial_lr / (1 + 0.05 * epoch)\n",
    "        for layer in network.layers:\n",
    "            layer.learning_rate = current_lr\n",
    "\n",
    "        # Shuffle training data\n",
    "        indices = np.random.permutation(num_samples)\n",
    "\n",
    "        # Mini-batch training\n",
    "        for i in range(0, num_samples, batch_size):\n",
    "            batch_indices = indices[i:i + batch_size]\n",
    "\n",
    "            # Prepare batch data\n",
    "            X_batch = np.array([np.asarray(train[idx][0]).flatten() / 255.0 for idx in batch_indices])\n",
    "            y_batch = np.zeros((len(batch_indices), 10))\n",
    "            for j, idx in enumerate(batch_indices):\n",
    "                y_batch[j, train[idx][1]] = 1\n",
    "\n",
    "            # Train batch and accumulate loss\n",
    "            loss = network.train_batch(X_batch, y_batch)\n",
    "            total_loss += loss\n",
    "            batch_count += 1\n",
    "\n",
    "        # Calculate and store average loss\n",
    "        avg_loss = total_loss / batch_count\n",
    "        losses.append(avg_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Avg Loss: {avg_loss:.4f}, LR: {current_lr:.6f}\")\n",
    "\n",
    "    return losses\n",
    "\n",
    "def test_network(network, test, num_samples=1000):\n",
    "    # Randomly select test samples\n",
    "    test_indices = np.random.choice(len(test), num_samples, replace=False)\n",
    "\n",
    "    # Prepare test data\n",
    "    X_test = np.array([np.asarray(test[idx][0]).flatten() / 255.0 for idx in test_indices])\n",
    "    y_test = np.zeros((len(test_indices), 10))\n",
    "    for j, idx in enumerate(test_indices):\n",
    "        y_test[j, test[idx][1]] = 1\n",
    "\n",
    "    # Evaluate model\n",
    "    test_loss, accuracy = network.evaluate(X_test, y_test)\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    return test_loss, accuracy\n",
    "\n",
    "# Define the network layers and activation functions\n",
    "layers = [\n",
    "    LinearLayer(28 * 28, 128, learning_rate=0.01), # Input und Hidden Layer\n",
    "    LinearLayer(128, 64, learning_rate=0.01),      # Hidden und Hidden Layer\n",
    "    LinearLayer(64, 10, learning_rate=0.01)        # Hidden und Output Layer\n",
    "]\n",
    "\n",
    "activations = [ReLU(), ReLU(), None]\n",
    "\n",
    "# Initialize the neural network\n",
    "neural_network = NeuralNetwork(layers=layers, activation_functions=activations)\n",
    "\n",
    "# Train the network\n",
    "loss_history = train_network(neural_network, train_dataset, num_epochs=10)\n",
    "\n",
    "# Test the network\n",
    "test_loss, test_accuracy = test_network(neural_network, test_dataset)"
   ],
   "id": "e1b71952059c7551"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Show Sample",
   "id": "7c79c8d599615766"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def test_sample(network, sample_idx=101):\n",
    "    # Prepare sample\n",
    "    X = np.asarray(train_dataset[sample_idx][0]).flatten() / 255.0\n",
    "    true_label = train_dataset[sample_idx][1]\n",
    "\n",
    "    # Get prediction\n",
    "    prediction = network.forward(X)\n",
    "    probabilities = softmax(prediction)\n",
    "    predicted_label = np.argmax(probabilities)\n",
    "\n",
    "    print(f\"True label: {true_label}\")\n",
    "    print(f\"Predicted probabilities: {probabilities[0]}\")\n",
    "    print(f\"Predicted label: {predicted_label}\")\n",
    "\n",
    "    # Show image\n",
    "    fig, ax = plt.subplots(figsize=(3, 3))\n",
    "    ax.imshow(train_dataset[sample_idx][0], cmap='gray')\n",
    "    ax.set_title(f\"Label: {true_label}, Predicted: {predicted_label}\")\n",
    "    ax.axis('off')\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "test_sample(neural_network)"
   ],
   "id": "1a41a508e5871e5f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Aufgabe 4",
   "id": "2cb7769491346164"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Modelle laufen lassen",
   "id": "23fe885454b1c9d4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define combinations of learning rates and hidden layer sizes\n",
    "learning_rates = [0.01, 0.1, 0.5, 1.0]\n",
    "hidden_sizes = [4, 8, 16]\n",
    "\n",
    "results = {}\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for size in hidden_sizes:\n",
    "        print(f\"Training with learning rate: {lr}, hidden layer size: {size}\")\n",
    "\n",
    "        # Define network layers\n",
    "        layers = [\n",
    "            LinearLayer(28 * 28, size, learning_rate=lr),   # Input und Hidden Layer\n",
    "            LinearLayer(size, 10, learning_rate=lr)         # Hidden und Output Layer\n",
    "        ]\n",
    "\n",
    "        activations = [ReLU(), None]\n",
    "\n",
    "        # Initialize the neural network\n",
    "        network = NeuralNetwork(layers=layers, activation_functions=activations)\n",
    "\n",
    "        # Train the network\n",
    "        train_losses = train_network(network, train_dataset, num_epochs=10)\n",
    "\n",
    "        # Test the network\n",
    "        test_loss, test_accuracy = test_network(network, test_dataset)\n",
    "\n",
    "        # Store results\n",
    "        results[(lr, size)] = {\n",
    "            'train_losses': train_losses,\n",
    "            'test_loss': test_loss,\n",
    "            'test_accuracy': test_accuracy\n",
    "        }"
   ],
   "id": "c47eb3ed285e1a9a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Visualisierung",
   "id": "35d30ef267d8a974"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "# Training Loss über Epochen\n",
    "plt.figure(figsize=(10, 6))\n",
    "for key, value in results.items():\n",
    "    plt.plot(value['train_losses'], label=f\"LR={key[0]}, Hidden={key[1]}\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Training Loss\")\n",
    "plt.title(\"Training Loss across Epochs\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Test Loss als Bar Chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "for key, value in results.items():\n",
    "    plt.bar(f\"LR={key[0]}, Hidden={key[1]}\", value['test_loss'])\n",
    "plt.ylabel(\"Test Loss\")\n",
    "plt.title(\"Test Loss after Training\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "# Test Accuracy als Bar Chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "for key, value in results.items():\n",
    "    plt.bar(f\"LR={key[0]}, Hidden={key[1]}\", value['test_accuracy'])\n",
    "plt.ylabel(\"Test Accuracy\")\n",
    "plt.title(\"Test Accuracy after Training\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ],
   "id": "64badad9524c3c5d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Aufgabe 5",
   "id": "39406e1705652194"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "def train_and_evaluate(learning_rates, hidden_sizes, num_epochs=10, batch_size=128):\n",
    "    results = {}\n",
    "\n",
    "    for lr in learning_rates:\n",
    "        for size in hidden_sizes:\n",
    "            print(f\"Training with learning rate: {lr}, hidden layer size: {size}\")\n",
    "\n",
    "            # network layers + hidden layers\n",
    "            layers = [\n",
    "                LinearLayer(28 * 28, size, learning_rate=lr),\n",
    "                LinearLayer(size, size, learning_rate=lr),\n",
    "                LinearLayer(size, size, learning_rate=lr),\n",
    "                LinearLayer(size, 10, learning_rate=lr)\n",
    "            ]\n",
    "\n",
    "            activations = [ReLU(), ReLU(), ReLU(), None]\n",
    "\n",
    "            # Initialize the neural network\n",
    "            network = NeuralNetwork(layers=layers, activation_functions=activations)\n",
    "\n",
    "            # Train the network\n",
    "            train_losses = train_network(network, train_dataset, num_epochs=num_epochs, batch_size=batch_size, initial_lr=lr)\n",
    "\n",
    "            # Test the network\n",
    "            test_loss, test_accuracy = test_network(network, test_dataset)\n",
    "\n",
    "            # Store results\n",
    "            results[(lr, size)] = {\n",
    "                'train_losses': train_losses,\n",
    "                'test_loss': test_loss,\n",
    "                'test_accuracy': test_accuracy\n",
    "            }\n",
    "    return results\n",
    "\n",
    "# learning rates and hidden layer sizes\n",
    "learning_rates = [0.001, 0.01, 0.1]\n",
    "hidden_sizes = [16, 32, 64]\n",
    "\n",
    "# Train and evaluate models\n",
    "results = train_and_evaluate(learning_rates, hidden_sizes, num_epochs=10)\n",
    "\n",
    "# Find the best performing model\n",
    "best_config = max(results, key=lambda x: results[x]['test_accuracy'])\n",
    "best_lr, best_size = best_config\n",
    "best_accuracy = results[best_config]['test_accuracy']\n",
    "\n",
    "print(f\"Best configuration: Learning rate={best_lr}, Hidden layer size={best_size}, Accuracy={best_accuracy:.4f}\")\n",
    "\n",
    "# Plot training losses\n",
    "for (lr, size), res in results.items():\n",
    "    plt.plot(res['train_losses'], label=f\"lr={lr}, size={size}\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Training Loss Over Epochs\")\n",
    "plt.show()\n"
   ],
   "id": "ca7d7a1ce4787c38"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
